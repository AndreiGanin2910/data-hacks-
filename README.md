# data-hacks-
Полное руководство по Data Science: теория и интуиция

1. EDA (Exploratory Data Analysis)

    Что это и зачем

EDA — это первый и критически важный этап любого проекта. Это исследование данных до построения моделей, чтобы понять их структуру, найти закономерности и проблемы.

    Ключевые задачи EDA

      Понимание структуры данных:      
- Сколько строк и столбцов
- Какие типы переменных (числовые, категориальные, временные)
- Какой таргет и как он распределён

      Оценка качества данных:      
- Где и сколько пропусков, случайны ли они
- Есть ли дубликаты
- Корректны ли типы данных
- Есть ли выбросы и аномалии

      Поиск закономерностей:      
- Как признаки связаны с таргетом
- Какие признаки коррелируют между собой
- Есть ли нелинейные зависимости
- Какие группы данных ведут себя по-разному

      Выявление проблем:      
- Утечка данных (data leakage) — признаки, которые "знают" таргет
- Несбалансированность классов
- Сдвиг распределений между train и test

    Почему EDA критически важен

     Без EDA вы рискуете:
- Построить модель на грязных данных
- Пропустить важные закономерности
- Создать признаки, которые не имеют смысла
- Выбрать неподходящую модель или метрику

      Правило: на EDA должно уходить 30-50% времени проекта. Это не потеря времени — это инвестиция в качество решения.

    

2. Модели машинного обучения

    2.1 Линейные модели

     Линейная регрессия

      Суть: Предполагает, что таргет — это взвешенная сумма признаков плюс константа. Модель находит оптимальные веса, минимизируя сумму квадратов ошибок.

      Когда хороша:      
- Есть линейная зависимость между признаками и таргетом
- Нужна интерпретируемость (каждый коэффициент показывает влияние признака)
- Мало данных (не переобучается)
- Нужен быстрый baseline

      Ограничения:      
- Не ловит нелинейные зависимости
- Чувствительна к выбросам
- Предполагает независимость признаков (мультиколлинеарность — проблема)

     Регуляризованные модели

      Ridge (L2-регуляризация):      
Штрафует большие коэффициенты, делая их меньше, но не обнуляет. Хороша когда все признаки потенциально важны, но нужно избежать переобучения.

      Lasso (L1-регуляризация):      
Может обнулять коэффициенты, фактически выполняя отбор признаков. Хороша когда подозреваете, что многие признаки бесполезны.

      ElasticNet:      
Комбинация Ridge и Lasso. Используется когда не уверены, какой тип регуляризации лучше.

     Логистическая регрессия

      Суть: Несмотря на название, это модель классификации. Предсказывает вероятность принадлежности к классу через сигмоидную функцию.

      Когда хороша:      
- Бинарная классификация
- Нужны калиброванные вероятности (не просто метка класса)
- Важна интерпретируемость
- Признаки линейно разделяют классы

      Важно: Часто работает лучше, чем ожидается. Всегда стоит попробовать как baseline.

    2.2 Support Vector Machines (SVM)

      Суть: Ищет гиперплоскость, которая максимально разделяет классы. Фокусируется на "опорных векторах" — точках, ближайших к границе.

      Kernel trick: Позволяет находить нелинейные границы, проецируя данные в пространство большей размерности без явного вычисления этой проекции.

      Когда хорош:      
- Высокая размерность данных (например, текст после TF-IDF)
- Чёткая граница между классами
- Небольшие и средние датасеты
- Бинарная классификация

      Ограничения:      
- Плохо масштабируется на большие данные (O(n²) - O(n³) по памяти)
- Требует масштабирования признаков
- Сложно интерпретировать с нелинейным ядром
- Чувствителен к выбору ядра и параметров

      Типичные ядра:      
- Linear — для линейно разделимых данных, быстрый
- RBF (Gaussian) — универсальный, хорошо работает по умолчанию
- Polynomial — когда важны взаимодействия признаков

    

    2.3 K-Nearest Neighbors (KNN)

      Суть: Классифицирует объект по большинству среди K ближайших соседей. Регрессия — усреднение значений соседей. Модель не "обучается" — просто запоминает данные.

      Когда хорош:      
- Простые задачи с локальными паттернами
- Мало данных
- Нужна интерпретируемость (можно показать похожие примеры)
- Как baseline
- Multi-label классификация

      Ограничения:      
- Медленный на больших данных (нужно считать расстояние до всех точек)
- Проклятие размерности — в высоких размерностях все точки "далеко"
- Чувствителен к масштабу признаков
- Чувствителен к нерелевантным признакам

      Выбор K:      
- Малое K — модель гибкая, риск переобучения
- Большое K — модель устойчивая, риск недообучения
- Обычно нечётное для бинарной классификации

    

    2.4 Naive Bayes

      Суть: Применяет теорему Байеса с "наивным" предположением о независимости признаков. Несмотря на нереалистичность предположения, часто работает хорошо.

      Варианты:      
- Gaussian NB       — для непрерывных признаков с нормальным распределением
- Multinomial NB       — для счётчиков (частоты слов в тексте)
- Bernoulli NB       — для бинарных признаков

      Когда хорош:      
- Классификация текстов (спам, тональность) — классика
- Очень быстрое обучение и предсказание
- Мало данных (не склонен к переобучению)
- Нужен baseline для классификации
- Много признаков

      Ограничения:      
- Предположение независимости редко выполняется
- Плохо калиброванные вероятности
- Не ловит сложные зависимости

      Интересно: Часто работает как отличный baseline. Если Naive Bayes даёт 90%, значит задача относительно простая.

    

    2.5 Деревья решений и Random Forest

     Деревья решений

      Суть: Модель последовательно разбивает данные по признакам, задавая вопросы типа "признак X больше порога T?". Каждый лист дерева содержит предсказание.

      Преимущества:      
- Интуитивно понятны, можно визуализировать
- Автоматически находят нелинейные зависимости и взаимодействия признаков
- Не требуют масштабирования данных
- Работают с категориальными признаками

      Главный недостаток: Склонны к переобучению. Одно дерево запоминает шум в данных.

     Random Forest

      Суть: Ансамбль из множества деревьев, где каждое дерево обучается на случайной подвыборке данных (bootstrap) и случайном подмножестве признаков. Итоговое предсказание — усреднение (регрессия) или голосование (классификация).

      Почему это работает:      
-  Bootstrap       добавляет разнообразие — деревья видят разные данные
-  Случайные признаки       делают деревья некоррелированными
-  Усреднение       уменьшает дисперсию и переобучение

      Когда использовать:      
- Универсальный алгоритм, работает "из коробки"
- Хорош для первого серьёзного baseline после линейных моделей
- Когда важна устойчивость и не критична максимальная точность
- Нужна оценка важности признаков

      Ограничения:      
- Медленнее градиентного бустинга на больших данных
- Не экстраполирует (предсказания ограничены диапазоном обучающих данных)
- На очень больших данных уступает бустингу по качеству

    

    2.6 Gradient Boosting (XGBoost, LightGBM, CatBoost)

     Общий принцип

      Суть: В отличие от Random Forest, где деревья независимы, здесь деревья строятся последовательно. Каждое новое дерево учится предсказывать ошибки всех предыдущих. Это как исправление работы — каждый следующий проход фокусируется на том, что ещё не исправлено.

      Почему это мощно:      
- Каждое дерево специализируется на "сложных" примерах
- Постепенно уменьшает как bias, так и variance
- Очень гибкая модель с множеством параметров настройки

     XGBoost

      Особенности:      
- Первая широко популярная реализация
- Оптимизированная работа с памятью
- Регуляризация встроена в алгоритм
- Автоматическая обработка пропусков

      Когда использовать:      
- Классика для табличных данных
- Соревнования на Kaggle (исторически доминировал)
- Когда нужен баланс скорости и качества

     LightGBM

      Особенности:      
-  Leaf-wise рост вместо level-wise — дерево растёт в глубину, выбирая лист с максимальным приростом качества. Быстрее, но риск переобучения выше.
- Histogram-based splitting — признаки разбиваются на бины, что драматически ускоряет обучение
- Нативная работа с категориальными признаками

      Когда использовать:      
- Большие датасеты (быстрее всех)
- Много признаков
- Когда скорость критична

      Важно: Требует более аккуратной настройки из-за склонности к переобучению.

     CatBoost

      Особенности:      
- Ordered Target Encoding — умное кодирование категорий, которое избегает утечки данных
- Ordered Boosting — борется с target leakage при построении деревьев
- Симметричные деревья — одинаковая структура на каждом уровне

      Когда использовать:      
- Много категориальных признаков (особенно высококардинальных)
- Не хотите тратить время на кодирование категорий
- Нужна устойчивость к переобучению "из коробки"
- Меньше данных (CatBoost менее склонен к переобучению)

     Сравнение и выбор

| Ситуация | Лучший выбор |
| Много категорий | CatBoost |
| Большие данные, важна скорость | LightGBM |
| Нужна стабильность | CatBoost или XGBoost |
| Мало данных | CatBoost |
| Первый эксперимент | Любой, но CatBoost проще |

      Общий совет: В реальных проектах попробуйте все три и сравните. Разница в качестве обычно невелика, но может быть значимой для конкретной задачи.

    2.7 Нейронные сети для табличных данных

      Многослойный перцептрон (MLP):      
Полносвязная сеть из нескольких слоёв. Каждый нейрон связан со всеми нейронами следующего слоя.

      Когда использовать:      
- Очень много данных (миллионы строк)
- Сложные нелинейные зависимости
- Хотите единую архитектуру для разных типов входов (табличные + текст + изображения)

      Когда НЕ использовать:      
- Мало данных — бустинг победит
- Нужна интерпретируемость
- Ограничены ресурсы

      TabNet, NODE, FT-Transformer:      
Специализированные архитектуры для табличных данных. Используют attention, деревья внутри сети. Иногда бьют бустинг, но требуют больше усилий.

      Практика: На табличных данных градиентный бустинг почти всегда лучше или не хуже нейросетей при меньших усилиях. Нейросети для таблиц — скорее исключение.

    

    2.8 Специализированные модели

     Модели для ранжирования (Learning to Rank)

      Задача: Упорядочить объекты по релевантности. Поиск, рекомендации, реклама.

      Подходы:      

      Pointwise: Предсказываем релевантность каждого объекта независимо. По сути обычная регрессия/классификация. Просто, но не оптимизирует ранжирование напрямую.

      Pairwise: Учимся определять, какой из двух объектов релевантнее. LambdaMART (в LightGBM, XGBoost) — самый популярный. Оптимизирует порядок пар.

      Listwise: Оптимизирует метрику ранжирования (NDCG) напрямую для всего списка. Сложнее, но теоретически лучше.

      Когда использовать:      
- Поисковые системы
- Рекомендации (какой товар показать выше)
- Реклама (какое объявление показать)

Модели для выживаемости (Survival Analysis)

      Задача: Предсказать время до события (отток клиента, поломка оборудования, смерть пациента). Учитывает цензурирование — когда событие не произошло за время наблюдения.

      Модель Кокса (Cox Proportional Hazards):      
Оценивает влияние признаков на "риск" события. Не предполагает конкретное распределение времени.

      Kaplan-Meier:      
Непараметрическая оценка функции выживания. Для визуализации и сравнения групп.

      Random Survival Forest:      
Адаптация Random Forest для выживаемости.

      Когда использовать:      
- Предсказание оттока с учётом времени
- Предиктивное обслуживание оборудования
- Медицинские исследования
- Когда важно не просто "произойдёт ли", а "когда"

     Модели для аномалий (Anomaly Detection)

      Задача: Найти необычные, нетипичные объекты. Фрод, сбои, дефекты.

      Isolation Forest:      
Строит деревья, которые "изолируют" точки. Аномалии изолируются быстрее (за меньшее число разбиений). Хорошо масштабируется, работает с высокой размерностью.

      One-Class SVM:      
Находит границу, охватывающую "нормальные" данные. Аномалии — за границей.

      Local Outlier Factor (LOF):      
Сравнивает локальную плотность точки с плотностью соседей. Аномалия — точка в разреженной области рядом с плотной.

      Autoencoders:      
Нейросеть учится сжимать и восстанавливать данные. Аномалии плохо восстанавливаются (большая ошибка реконструкции).

      Когда какой:      
- Много данных, нужна скорость → Isolation Forest
- Сложная форма "нормальной" области → One-Class SVM
- Локальные аномалии среди кластеров → LOF
- Очень сложные данные, много ресурсов → Autoencoders

     Модели для кластеризации

      K-Means:      
Разбивает данные на K кластеров, минимизируя расстояние до центров. Быстрый, простой. Требует задать K заранее, находит только выпуклые кластеры.

      DBSCAN:      
Находит кластеры как плотные области, разделённые разреженными. Не требует задавать K, находит кластеры произвольной формы, умеет находить выбросы. Чувствителен к параметрам плотности.

      Hierarchical Clustering:      
Строит дерево (дендрограмму) вложенных кластеров. Можно выбрать число кластеров после построения. Интерпретируемо, но не масштабируется.

      Gaussian Mixture Models (GMM):      
Предполагает, что данные — смесь нормальных распределений. Мягкая кластеризация (вероятность принадлежности). Хорош для перекрывающихся кластеров.

      Выбор числа кластеров:      
- Elbow method — ищем "локоть" на графике inertia(K)
- Silhouette score — качество разделения
- Domain knowledge — часто знаем сколько групп ожидать

     Модели для многоклассовой и многометочной классификации

      Multiclass (один класс из многих):      
- One-vs-Rest (OvR): K бинарных классификаторов, каждый отличает один класс от остальных
- One-vs-One (OvO): K(K-1)/2 классификаторов для каждой пары классов
- Нативная поддержка: Деревья, нейросети, Naive Bayes работают с многими классами напрямую

      Multilabel (несколько меток одновременно):      
- Binary Relevance: Независимый классификатор для каждой метки. Просто, но игнорирует зависимости между метками
- Classifier Chains: Классификаторы по цепочке, каждый использует предсказания предыдущих как признаки
-  Нейросети: Sigmoid на выходе вместо softmax

    

3. Валидация

    Зачем нужна валидация

Модель должна работать на       новых       данных, которые она не видела. Валидация имитирует это: мы прячем часть данных, обучаем модель на остальном, и проверяем на спрятанном.

      Без правильной валидации:      
- Не знаете реальное качество модели
- Рискуете переобучиться при подборе параметров
- Выберете неоптимальную модель

    Hold-out

      Суть: Просто делим данные на train и test (и иногда validation).

      Плюсы: Быстро, просто.

      Минусы: Оценка нестабильна — зависит от того, как именно разделили. Теряем данные для обучения.

      Когда использовать: Очень много данных, нужна скорость.

    K-Fold Cross-Validation

      Суть: Делим данные на K частей (фолдов). По очереди каждый фолд становится валидационным, остальные — обучающими. Получаем K оценок, усредняем.

      Плюсы:      
- Стабильная оценка (усреднение)
- Используем все данные и для обучения, и для валидации
- Видим разброс качества (важно!)

      Минусы: В K раз дольше.

      Типичные значения K: 5 или 10. Больше — стабильнее оценка, но дольше.

    Stratified K-Fold

      Суть: То же самое, но сохраняет пропорции классов в каждом фолде.

      Когда использовать: Классификация, особенно при дисбалансе классов. Всегда используйте для классификации.      

    Group K-Fold

      Суть: Гарантирует, что объекты одной группы не попадают одновременно в train и validation.

      Когда использовать:      
- Несколько записей от одного пользователя
- Несколько транзакций одного клиента
- Временные окна одного объекта

      Почему важно: Если один пользователь попадёт в train и validation, модель может "запомнить" его, и оценка будет оптимистичной.

    Time Series Split

      Суть: Обучаемся на прошлом, предсказываем будущее. Каждый следующий фолд добавляет данные в обучение.

      Когда использовать: Временные ряды, любые данные с временной зависимостью.

      Почему важно: Обычный K-Fold для временных рядов — грубая ошибка. Модель "увидит будущее" и оценка будет нереалистично хорошей.

    Expanding Window vs Sliding Window

      Expanding Window:      
Обучение на всех данных до момента T, тест на T+1. Каждый фолд добавляет данные.

      Sliding Window:      
Фиксированное окно обучения, которое сдвигается. Не использует старые данные.

      Когда какой:      
- Expanding: когда старые данные всегда полезны
- Sliding: когда паттерны меняются, старые данные могут мешать

    Выбор стратегии

      Главный принцип: Валидация должна имитировать реальное использование модели.

- Если модель будет предсказывать для новых пользователей → Group K-Fold по пользователям
- Если модель будет предсказывать будущее → Time Series Split
- Если всё случайно → Stratified K-Fold (для классификации) или обычный K-Fold

    

4. Работа с данными

    4.1 Пропуски

      Типы пропусков:      
-  MCAR (Missing Completely at Random): Пропуск не зависит ни от чего. Можно удалять.
-  MAR (Missing at Random): Пропуск зависит от других признаков. Можно восстановить.
-  MNAR (Missing Not at Random): Пропуск зависит от самого значения. Сам факт пропуска — информация!

      Стратегии:      
-       Удаление — если пропусков мало (<5%) и они MCAR
-       Заполнение константой — просто, но теряет информацию
-       Заполнение статистикой (медиана, мода) — baseline подход
-       Заполнение по группам— если значение зависит от категории
-       Модельное заполнение (KNN, предсказание) — мощнее, но риск утечки
-       Индикатор пропуска— создать признак "было пропущено"

      Важно: Для деревьев и бустинга (особенно CatBoost) пропуски часто лучше оставить как есть — алгоритмы умеют с ними работать.

    4.2 Категориальные признаки

      Label Encoding:      
Присваиваем каждой категории число. Работает для порядковых признаков (низкий < средний < высокий) или древесных моделей.

      One-Hot Encoding:      
Создаём отдельный бинарный признак для каждой категории. Хорошо для линейных моделей и небольшого числа категорий.

      Target Encoding:      
Заменяем категорию средним таргетом по ней. Мощно, но риск переобучения и утечки. Нужна регуляризация.

      Target Encoding с регуляризацией:      
Редкие категории "притягиваются" к глобальному среднему через сглаживание. K-Fold Target Encoding избегает утечки.

      Frequency/Count Encoding:      
Заменяем категорию частотой или количеством её встречаемости. Безопасно, часто работает хорошо.

      Для высококардинальных признаков (много уникальных значений):      
- Target Encoding с регуляризацией
- Оставить топ-N категорий, остальные в "Other"
- Embedding (для нейросетей)

    4.3 Даты

Даты — это кладезь признаков:

      Базовые: год, месяц, день, день недели, час, минута

      Производные:      
- Выходной / рабочий день
- Праздник
- Начало/конец месяца, квартала
- Номер недели, день года

      Циклические: Месяц и час "закольцованы" (декабрь близок к январю). Кодируем через sin/cos для сохранения этой близости.

      Относительные:      
- Дней с последней покупки
- Дней до события
- Возраст записи

    4.4 Выбросы

      Методы обнаружения:      
-  IQR метод: Значения за пределами [Q1 - 1.5·IQR, Q3 + 1.5·IQR]
-  Z-score: Значения с |z| > 3
-  Визуальный анализ: Boxplot, scatter plot

      Что делать:      
- Удалить — если это явные ошибки данных
-  Ограничить (clipping) — заменить на граничное значение
-  Оставить — если это реальные редкие значения (деревья справятся)
-  Логарифмировать — если распределение скошено

    4.5 Feature Engineering

      Агрегации по группам:      
Статистики — среднее, сумма, количество, std покупок пользователя.

      Взаимодействия признаков:      
Произведение, отношение, разность признаков. Цена за единицу = цена / количество.

      Полиномиальные признаки:      
Квадраты, кубы признаков. Помогает линейным моделям ловить нелинейность.

      Биннинг:      
Разбиение числового признака на интервалы. Возраст → возрастная группа.

      Частотные признаки:      
- Frequency encoding: частота категории
- Count encoding: абсолютное количество

      Оконные агрегации (для временных данных):      
- Сумма за последние 7 дней
- Среднее за последний месяц
- Тренд (сравнение недель)

    4.6 Географические данные

      Базовые признаки:      
- Широта, долгота
- Расстояние до центра города / важных точек
- Геохеш (дискретизация координат)

      Агрегации по геозонам:      
- Средняя цена недвижимости в районе
- Плотность населения
- Количество объектов инфраструктуры рядом

    4.7 Последовательности событий

      Для каждой последовательности:      
- Длина последовательности
- Время от первого до последнего события
- Среднее/медианное время между событиями
- Количество уникальных типов событий
- N-граммы событий
- Последнее событие, первое событие

    

5. Подбор гиперпараметров

    Grid Search

      Суть: Перебираем все комбинации параметров из заданной сетки.

      Плюсы: Гарантированно найдёт лучшую комбинацию из сетки.

      Минусы: Экспоненциально растёт с числом параметров. 5 параметров по 5 значений = 3125 комбинаций.

      Когда использовать: Мало параметров, есть время, нужна воспроизводимость.

    Random Search

      Суть: Случайно выбираем комбинации параметров из заданных распределений.

      Почему работает:      
- Если хороших комбинаций много, случайно найдём быстро
- Покрывает пространство более равномерно чем сетка
- Исследования показывают: часто не хуже Grid Search при меньшем числе итераций

      Когда использовать: Много параметров, ограничено время.

    Bayesian Optimization (Optuna, Hyperopt)

      Суть: Строит модель функции "параметры → качество" и использует её для выбора следующей точки. Балансирует между исследованием новых областей и эксплуатацией известных хороших.

      Почему это мощно:      
- Учится на предыдущих экспериментах
- Быстрее находит хорошие области
- Умеет работать с условными параметрами
- Может останавливать неперспективные эксперименты рано (pruning)

      Optuna особенно хороша:      
- Простой интерфейс
- Визуализация
- Pruning из коробки
- Параллелизация

      Когда использовать: Много параметров, дорогие эксперименты (долго обучается модель).

    Практические советы

      Порядок важности параметров для бустинга:      
1. Learning rate (самый важный)
2. Число деревьев (используйте early stopping)
3. Глубина деревьев / число листьев
4. Регуляризация
5. Подвыборка данных и признаков

      Стратегия: Сначала грубый поиск (широкие диапазоны), потом уточнение вокруг лучших значений.

    

6. Ансамблирование

    Bagging vs Boosting vs Stacking

      Bagging (Random Forest):      
- Параллельное обучение независимых моделей
- Уменьшает variance
- Устойчив к переобучению
- Хорош когда базовая модель переобучается

      Boosting (Gradient Boosting):      
- Последовательное обучение, каждая модель исправляет предыдущие
- Уменьшает bias и variance
- Риск переобучения при слишком многих итерациях
- Обычно даёт лучшее качество

      Stacking:      
- Обучает мета-модель комбинировать предсказания
- Может использовать разные типы базовых моделей
- Сложнее реализовать правильно

    Blending (усреднение)

      Суть: Просто усредняем предсказания нескольких моделей.

      Почему работает:      
- Разные модели делают разные ошибки
- При усреднении ошибки частично компенсируются
- Уменьшается дисперсия предсказаний

      Виды:      
- Простое среднее — все модели равноважны
- Взвешенное среднее — лучшим моделям больший вес
- Rank averaging — усредняем ранги, не значения. Устойчиво к разным масштабам.

      Важно: Лучше всего работает когда модели разные (разные алгоритмы, разные признаки). Усреднение одинаковых моделей даёт мало.

    Stacking

      Суть: Предсказания базовых моделей становятся признаками для мета-модели, которая учится их комбинировать.

      Почему мощнее blending:      
- Мета-модель может выучить нелинейные комбинации
- Может понять, когда какая модель надёжнее

      Критически важно: Использовать OOF (Out-of-Fold) предсказания! Иначе мета-модель переобучится на идеальных предсказаниях базовых моделей.

      Типичная мета-модель: Линейная регрессия или логистическая регрессия. Простая модель, чтобы не переобучиться.

    Multi-level Stacking

      Структура:      
- Level 0: Базовые модели
- Level 1: Мета-модели на предсказаниях Level 0
- Level 2: Финальная модель

      Риски: Каждый уровень — риск переобучения. Редко оправдано вне соревнований.

    Pseudo-labeling

      Суть: Используем предсказания модели на тестовых (или неразмеченных) данных как дополнительные обучающие примеры.

      Когда помогает:      
- Мало размеченных данных, много неразмеченных
- Модель уже достаточно хороша
- Распределение train и test похожи

      Риски:      
- Если модель ошибается, ошибки усилятся
- Нужно брать только "уверенные" предсказания

      Как снизить риски:      
- Порог уверенности (брать только примеры с высокой вероятностью)
- Итеративный подход (постепенно добавлять примеры)

    Diversity в ансамблях

      Принцип: Ансамбль эффективен, когда модели делают разные ошибки.

      Как добиться:      
- Разные алгоритмы (LightGBM + CatBoost + Neural Net)
- Разные подмножества признаков
- Разные подмножества данных
- Разные гиперпараметры
- Разные seed'ы (даже это даёт небольшое разнообразие)

    

7. Временные ряды

    Особенности

Временные ряды принципиально отличаются от обычных табличных данных:
- Порядок важен — нельзя перемешивать
- Автокорреляция — значение зависит от прошлых значений
- Тренд — долгосрочное направление
- Сезонность — периодические паттерны
- Нестационарность — свойства ряда меняются со временем

    Stationarity (Стационарность)

      Что это: Статистические свойства ряда не меняются со временем (среднее, дисперсия, автокорреляция постоянны).

      Почему важно: Классические модели (ARIMA) требуют стационарности.

      Как проверить:      
- ADF тест (Augmented Dickey-Fuller)
- Визуально: нет тренда, постоянный разброс

      Как достичь:      
- Дифференцирование (разности)
- Логарифмирование
- Удаление тренда

    Классические методы

      ARIMA (AutoRegressive Integrated Moving Average):      
- AR — зависимость от прошлых значений
- I — дифференцирование для стационарности
- MA — зависимость от прошлых ошибок

Хорошо работает для стационарных рядов, интерпретируема.

      SARIMA:      
ARIMA + сезонная компонента. Для рядов с выраженной сезонностью.

      Prophet (Facebook):      
Разложение на тренд + сезонность + праздники. Прост в использовании, автоматически обрабатывает праздники и выбросы. Хорош как baseline.

    ML подход

      Суть: Превращаем задачу прогнозирования в обычную регрессию с лаговыми признаками.

      Признаки:      
- Лаги: значение 1, 7, 14, 28 дней назад
- Скользящие статистики: среднее, std, min, max за окно
- Календарные: день недели, месяц, праздник
- Экспоненциальные скользящие средние

      Модели: Gradient Boosting работает отлично. Может ловить сложные нелинейные зависимости и взаимодействия.

      Критически важно: Правильная валидация! Только Time Series Split. Обучение на прошлом, тест на будущем.

    Экзогенные переменные

      Суть: Внешние факторы, влияющие на ряд (праздники, погода, маркетинговые акции).

      Важно: Для прогноза нужно знать будущие значения экзогенных переменных! Праздники знаем, погоду — можем взять прогноз.

    Многошаговый прогноз

      Рекурсивный: Предсказываем шаг 1, используем это предсказание для шага 2, и т.д. Ошибки накапливаются.

      Прямой: Отдельная модель для каждого горизонта. Больше моделей, но нет накопления ошибок.

    Множественные временные ряды

      Глобальные модели: Одна модель на все ряды. Учится на общих паттернах. LightGBM с признаками ряда отлично работает.

      Преимущество: Больше данных, перенос знаний между рядами, работает даже для коротких рядов.

    

   8. Рекомендательные системы (RecSys)

    Collaborative Filtering

      Идея: Похожие пользователи любят похожие вещи. Или: если пользователю нравятся вещи A и B, и другие пользователи, которым нравится A, также любят C, то предложим пользователю C.

      User-based: Находим похожих пользователей, рекомендуем что нравится им.

      Item-based: Находим похожие товары, рекомендуем похожее на то, что уже понравилось.

      Matrix Factorization (SVD): Раскладываем матрицу "пользователь-товар" на произведение двух матриц меньшей размерности. Каждый пользователь и товар представлены вектором в скрытом пространстве.

      Плюсы: Не нужны признаки товаров, работает на поведении.

      Минусы: Cold start — не можем рекомендовать новым пользователям или новые товары.

    Content-based

      Идея: Рекомендуем товары, похожие по характеристикам на то, что пользователь уже выбирал.

      Плюсы: Нет проблемы cold start для товаров, интерпретируемо.

      Минусы: Нужны хорошие признаки товаров, не открывает "неожиданные" рекомендации.

    Implicit Feedback

Когда нет явных оценок, но есть поведение: клики, покупки, время просмотра.

      ALS (Alternating Least Squares): Модифицированная матричная факторизация для неявных данных.

      BPR (Bayesian Personalized Ranking): Оптимизирует ранжирование: товар, с которым взаимодействовали, должен быть выше случайного.

    Метрики RecSys

-       Precision@K: Доля релевантных среди топ-K рекомендаций
-       Recall@K: Доля найденных релевантных из всех релевантных
-       NDCG: Учитывает позицию — релевантные выше важнее
-       MAP: Средняя точность по всем позициям
-       MRR: Средний обратный ранг первого релевантного

    

9. Computer Vision (CV)

    Transfer Learning

      Идея: Нейросети, обученные на огромных датасетах (ImageNet), выучили общие признаки изображений: края, текстуры, формы. Эти знания можно перенести на новую задачу.

      Feature extraction: Замораживаем предобученную сеть, используем её как извлекатель признаков, обучаем только последний слой.

      Fine-tuning: Размораживаем часть слоев (обычно последние) и дообучаем на своих данных с маленьким learning rate.

      Когда что:      
- Мало данных → Feature extraction
- Больше данных, задача похожа на ImageNet → Fine-tuning последних слоев
- Много данных, задача отличается → Fine-tuning всей сети или обучение с нуля

    Аугментации

      Суть: Искусственно увеличиваем датасет, применяя трансформации к изображениям.

      Базовые: Повороты, отражения, сдвиги, масштабирование, изменение яркости/контраста.

      Продвинутые: Cutout (вырезание частей), Mixup (смешивание изображений), CutMix (вставка части одного в другое).

      Почему важно:      
- Увеличивает эффективный размер датасета
- Модель становится устойчивее
- Критично при малом количестве данных

    Архитектуры

      ResNet: Революционная идея skip connections — градиент может "пропустить" слои. Позволило обучать очень глубокие сети.

      EfficientNet: Сбалансированное масштабирование ширины, глубины и разрешения. Оптимальный trade-off качество/скорость.

      Vision Transformer (ViT): Применение архитектуры трансформера к изображениям. Требует много данных, но даёт SOTA на больших датасетах.

    

10. NLP (Natural Language Processing)

    Классические методы

      Bag of Words: Текст — это набор слов без учёта порядка. Просто, интерпретируемо, работает для многих задач.

      TF-IDF: Взвешивает слова по важности. Редкие слова важнее частых. Улучшение над BoW.

      Word2Vec, FastText: Слова как векторы в пространстве, где похожие слова близко. "Король - мужчина + женщина ≈ королева".

      Ограничения: Не учитывают контекст. Слово имеет один вектор независимо от предложения.

    Трансформеры и BERT

      Революция: Attention механизм позволяет каждому слову "смотреть" на все другие слова в предложении. Контекст учитывается.

      BERT: Предобучен на огромных текстах, понимает контекст с обеих сторон. Fine-tuning на конкретную задачу.

      Когда использовать:      
- Сложные задачи NLP
- Нужно понимание смысла
- Есть ресурсы (GPU)

      Sentence Transformers: BERT, оптимизированный для получения эмбеддингов предложений. Отлично для semantic search, similarity.

    

11. LLM (Large Language Models)

    Что это

Огромные языковые модели (GPT-4, LLaMA, Claude), обученные на триллионах токенов. Могут генерировать текст, отвечать на вопросы, писать код, рассуждать.

    Prompt Engineering

      Zero-shot: Просто формулируем задачу. "Classify the sentiment: 'Great product!'"

      Few-shot: Даём несколько примеров перед вопросом. Модель учится на примерах "на лету".

      Chain of Thought: Просим модель рассуждать пошагово. "Let's think step by step". Улучшает качество на сложных задачах.

    RAG (Retrieval-Augmented Generation)

      Проблема: LLM знает только то, на чём обучена. Не знает ваши документы, актуальные данные.

      Решение:       
1. Разбиваем документы на куски
2. Создаём эмбеддинги для поиска
3. При вопросе находим релевантные куски
4. Подаём их в LLM вместе с вопросом

      Применения: Чат-боты по документации, поиск по базе знаний, Q&A системы.

    Fine-tuning LLM

      Полное fine-tuning: Дообучаем все параметры. Дорого, нужно много данных.

      LoRA (Low-Rank Adaptation): Добавляем маленькие обучаемые матрицы, основная модель заморожена. Эффективно, можно на consumer GPU.

      Когда fine-tuning:      
- Специфический стиль/формат ответов
- Узкая доменная область
- Prompt engineering недостаточно

    

12. Метрики

    Регрессия

      MSE/RMSE: Штрафует большие ошибки сильнее. Чувствителен к выбросам.

      MAE: Средняя абсолютная ошибка. Устойчивее к выбросам.

      MAPE: Процентная ошибка. Интерпретируема, но проблемы при значениях близких к нулю.

      R²: Доля объяснённой дисперсии. 1 = идеально, 0 = модель не лучше среднего.

      RMSLE: Логарифмическая ошибка. Хорошо когда важны относительные ошибки и есть выбросы.

    Классификация

      Accuracy: Доля правильных ответов. Бесполезна при дисбалансе классов!

      Precision: Из тех, кого назвали положительными, сколько действительно положительные. Важно когда цена false positive высока.

      Recall: Из реально положительных, сколько нашли. Важно когда нельзя пропустить (рак, фрод).

      F1: Гармоническое среднее precision и recall. Баланс между ними.

      ROC-AUC: Способность модели ранжировать. Не зависит от порога. Хорошо для сравнения моделей.

      PR-AUC: Площадь под precision-recall кривой. Лучше для несбалансированных данных.

    Ранжирование

      NDCG: Учитывает позицию — релевантные объекты выше важнее.

      MAP: Средняя точность по всем позициям.

      MRR: Средний обратный ранг первого релевантного.

    

13. Работа с несбалансированными данными

    Почему это проблема

При соотношении классов 99:1 модель может предсказывать всегда "0" и иметь 99% accuracy. Но она бесполезна для поиска редкого класса.

    Методы на уровне данных

      Oversampling (увеличение меньшинства):      
-       Random Oversampling: Дублируем примеры редкого класса. Просто, но риск переобучения.
-       SMOTE: Создаёт синтетические примеры, интерполируя между существующими. Добавляет разнообразие.
-       ADASYN: Фокусируется на "сложных" примерах, создаёт больше синтетики там.

      Undersampling (уменьшение большинства):      
-       Random Undersampling: Удаляем примеры частого класса. Теряем информацию.
-       Tomek Links: Удаляем пары близких примеров разных классов. Очищает границу.

      Комбинации: SMOTE + Tomek Links, SMOTE + ENN.

    Методы на уровне алгоритма

      Class weights: Присваиваем больший вес редкому классу в функции потерь. Поддерживается большинством алгоритмов.

      Threshold tuning: Вместо порога 0.5 подбираем оптимальный по метрике. Если нужен высокий recall — снижаем порог.

      Cost-sensitive learning: Разные штрафы за разные типы ошибок.

    Выбор метрики

      Не используйте accuracy!      

Хорошие метрики:
-       F1-score — баланс precision и recall
-       PR-AUC — площадь под precision-recall кривой
-       ROC-AUC — способность ранжировать (но может быть оптимистичной при сильном дисбалансе)
-       Отдельно precision и recall — если один важнее

    

14. Интерпретируемость моделей

    Почему важна интерпретируемость

- Доверие к модели (медицина, финансы, право)
- Отладка и улучшение модели
- Выявление утечки данных
- Регуляторные требования (GDPR "право на объяснение")
- Понимание предметной области

    Feature Importance

      Impurity-based (для деревьев):      
Сумма уменьшения примеси по всем разбиениям признака. Встроено в sklearn. Недостаток: смещено в пользу признаков с большим числом значений.

      Permutation Importance:      
Перемешиваем значения признака, смотрим насколько упало качество. Честнее, работает для любой модели.

      Drop Column Importance:      
Убираем признак, переобучаем модель, смотрим изменение качества. Точно, но очень долго.

    SHAP (SHapley Additive exPlanations)

      Суть: Из теории игр. Вклад каждого признака — его "справедливая" доля в предсказании. Учитывает взаимодействия.

      Что показывает:      
- Глобальную важность признаков
- Направление влияния (положительное/отрицательное)
- Влияние для конкретного предсказания
- Взаимодействия между признаками

      Когда использовать:      
- Нужно объяснить конкретное предсказание клиенту
- Хотите понять модель глубже
- Подозреваете проблемы с данными

    LIME (Local Interpretable Model-agnostic Explanations)

      Суть: Для конкретного предсказания строит простую интерпретируемую модель (линейную) локально вокруг точки.

      Отличие от SHAP: Быстрее, но менее теоретически обосновано. Хорош для быстрых объяснений.

    Partial Dependence Plots (PDP)

      Суть: Показывает среднее влияние признака на предсказание при фиксации других признаков.

      Для чего: Понять характер зависимости — линейная, монотонная, есть ли пороги.

    

15. Борьба с утечкой данных (Data Leakage)

    Типы утечек

      Target Leakage: Признак содержит информацию о таргете, которая недоступна в момент предсказания.

Примеры:
- Дата закрытия кредита для предсказания дефолта
- Среднее время в приложении после покупки для предсказания покупки
- ID менеджера, который назначается после классификации

      Train-Test Contamination: Информация из теста попадает в обучение.

Примеры:
- Нормализация по всему датасету до разбиения
- Target encoding по всем данным
- Заполнение пропусков средним по всему датасету

    Как обнаружить

- Слишком хорошее качество (если AUC = 0.99, что-то не так)
- Огромная важность неожиданного признака
- Большой разрыв между CV и LB (public leaderboard)

    Как избежать

- Думать о времени: был ли признак известен в момент предсказания?
- Все трансформации внутри CV (fit на train, transform на val)
- Проверять feature importance на подозрительные признаки
- Строить pipeline'ы

    

16. Калибровка вероятностей

    Проблема

Многие модели выдают скоры, которые не являются настоящими вероятностями. Если модель говорит "0.7", это не значит, что событие произойдёт в 70% случаев.

    Когда важна калибровка

- Когда решения принимаются на основе вероятностей (страхование, медицина)
- Когда вероятности сравниваются между моделями
- Когда вероятности используются в downstream задачах

    Методы калибровки

      Platt Scaling: Обучаем логистическую регрессию на скорах модели. Хорошо для небольшого количества классов.

      Isotonic Regression: Непараметрическая калибровка. Более гибкая, но требует больше данных.

      Temperature Scaling (для нейросетей): Делим logits на температуру T перед softmax. Простой и эффективный.

    Как проверить калибровку

      Reliability Diagram (Calibration Curve): По оси X — предсказанная вероятность, по Y — реальная доля положительных. Идеально — диагональ.

      Brier Score: Средний квадрат отклонения от истинной метки. Меньше — лучше.

    

17. AutoML

    Что это

Автоматизация процесса машинного обучения: выбор модели, feature engineering, подбор гиперпараметров.

    Популярные инструменты

      Auto-sklearn: Автоматический выбор из sklearn моделей + ансамблирование. Академический, хорошо документирован.

      H2O AutoML: Быстрый, production-ready. Хорош для baseline и прототипирования.

      AutoGluon: От Amazon. Сильный на табличных данных, автоматическое ансамблирование.

      TPOT: Генетические алгоритмы для поиска pipeline. Интересный подход, но медленный.

    Когда использовать

      Хорош для:      
- Быстрого baseline
- Когда не знаете, с чего начать
- Прототипирования
- Непрофильных задач ML

      Не замена человеку:      
- Не понимает бизнес-контекст
- Не делает осмысленный feature engineering
- Может пропустить утечку данных
- Сложно кастомизировать

    

18. Процессы и soft skills

    Работа с бизнесом

      Понимание задачи:      
- Какую проблему решаем?
- Как будет использоваться модель?
- Какова цена ошибки каждого типа?
- Какие ограничения (скорость, интерпретируемость)?

      Перевод метрик:      
- RMSE в $50 может ничего не значить для бизнеса
- "Снижение оттока на 5%" — понятнее

    Эксперименты и документация

      Ведите логи:      
- Что попробовали
- Какие результаты
- Какие гипотезы

      Воспроизводимость:      
- Фиксируйте random seeds
- Версионируйте код и данные
- Документируйте препроцессинг

    Мониторинг в production

      Data drift: Распределение входных данных меняется. Модель обучена на других данных.

      Concept drift: Связь между признаками и таргетом меняется. Модель устарела.

      Мониторинг:      
- Распределения входных признаков
- Распределение предсказаний
- Качество на новых размеченных данных
- Бизнес-метрики

    

19. Итоговые рекомендации

    Выбор модели по задаче

| Задача | Первый выбор | Альтернативы |
| Табличные данные | LightGBM / CatBoost | XGBoost, Random Forest |
| Мало данных | CatBoost, Linear models | Random Forest |
| Много категорий | CatBoost | LightGBM с encoding |
| Нужна интерпретируемость | Linear models, Decision Tree | SHAP для любой модели |
| Высокая размерность | SVM, Lasso | Random Forest |
| Текст (классика) | TF-IDF + LogReg/SVM | Naive Bayes |
| Текст (современный) | BERT, RoBERTa | DistilBERT |
| Изображения | EfficientNet, ResNet | Vision Transformer |
| Рекомендации | ALS, LightFM | Neural CF |
| Временные ряды | LightGBM с лагами | Prophet, ARIMA |
| Аномалии | Isolation Forest | Autoencoder, One-Class SVM |
| Ранжирование | LambdaMART (LightGBM) | Neural rankers |
| Кластеризация | K-Means, DBSCAN | GMM, Hierarchical |

    Порядок работы над проектом

1.       Понять задачу — бизнес-цель, как будет использоваться модель
2.       Определить метрику — что оптимизируем, как измеряем успех
3.       EDA — понять данные глубоко
4.       Baseline — простая модель (среднее, логистическая регрессия)
5.       Валидация — настроить правильную схему CV
6.       Feature engineering — создать признаки на основе EDA
7.       Моделирование — попробовать несколько алгоритмов
8.       Тюнинг — оптимизировать гиперпараметры лучших моделей
9.       Ансамбли — объединить модели
10.       Анализ ошибок — понять, где модель ошибается, улучшить

    Что действительно важно (в порядке убывания)

1.       Понимание задачи и данных — без этого всё остальное бессмысленно
2.       Качество данных  — garbage in, garbage out
3.       Правильная валидация — иначе не знаете реальное качество
4.       Feature engineering — даёт больше, чем смена модели
5.       Выбор правильной метрики — оптимизируйте то, что важно
6.       Правильный baseline— чтобы было с чем сравнивать
7.       Тюнинг гиперпараметров — обычно даёт 5-10% улучшения
8.       Ансамблирование — финальные проценты

    Частые ошибки

- Начинать со сложных моделей вместо baseline
- Тюнить модель до EDA и feature engineering
- Использовать неправильную валидацию
- Игнорировать дисбаланс классов
- Не проверять на утечку данных
- Оптимизировать accuracy при дисбалансе
- Не смотреть на ошибки модели
- Переусложнять, когда простое решение работает

    

Главное правило: Gradient Boosting (CatBoost/LightGBM) + хороший feature engineering + правильная валидация решают 90% табличных задач. Не нужно усложнять без необходимости.
